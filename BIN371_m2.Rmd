---
title: "BIN371_m2"
output: html_document
date: "2025-08-10"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

---

## 1. Overview (What This Milestone Delivers)

This document completes Milestone 2 by:
- Describing the selected datasets and final columns (**Data Description**).
- Justifying inclusion/exclusion rules (**Data Selection**).
- Cleaning and standardizing the data; reporting missingness, duplicates, and outliers (**Data Cleaning Process**).
- Running basic statistical tests to inform which attributes matter (**Attribute/Feature Selection**).
- Encoding, scaling, optional winsorization, and generating train/test splits (**Data Transformations & Aggregation**).

All cleaned CSVs and train/test matrices are written to `outputs_m2/`.

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
suppressWarnings(suppressMessages({
  if (!requireNamespace("tidyverse", quietly = TRUE)) install.packages("tidyverse")
  library(tidyverse)  # dplyr, tidyr, ggplot2, readr, purrr
}))
```

## 2. Load Data

```{r load-data}
# Adjust paths if your CSVs live elsewhere
paths <- list(
  Anthropometry     = "anthropometry_national_zaf.csv",
  Literacy          = "literacy_national_zaf.csv",
  MaternalMortality = "maternal-mortality_national_zaf.csv",
  ARI_Symptoms      = "symptoms-of-acute-respiratory-infection-ari_national_zaf.csv"
)

read_base <- function(p) read.csv(p, stringsAsFactors = FALSE, check.names = FALSE)
df_list_raw <- lapply(paths, read_base)
names(df_list_raw) <- names(paths)

# Output directory
out_dir <- "outputs_m2"
if (!dir.exists(out_dir)) dir.create(out_dir, recursive = TRUE)
```

## 3. Standardize Columns & Create `Value_num`

```{r standardize-and-numeric}
# Standardize names + parse Value -> Value_num (handles "12%", "1,234", etc.)
clean_df <- function(df){
  # unify column names
  names(df) <- gsub("\\s+", "_", names(df))
  names(df) <- gsub("[^A-Za-z0-9_]", "", names(df))
  # numeric extraction for Value
  if ("Value" %in% names(df)) {
    df$Value_num <- readr::parse_number(as.character(df$Value))
  }
  df
}

df_list <- lapply(df_list_raw, clean_df)

# Quick sanity: which datasets had non-numeric Value originally?
for (nm in names(df_list)) {
  df <- df_list[[nm]]
  if ("Value" %in% names(df)) {
    bad <- suppressWarnings(sum(is.na(as.numeric(df$Value)) & !is.na(df$Value)))
    cat(nm, "- non-numeric Value entries initially:", bad, "\n")
  }
}
```

## 4. Data Description (Current Shapes & Key Columns)

```{r data-description}
dims <- tibble(
  Dataset = names(df_list),
  Rows    = sapply(df_list, nrow),
  Columns = sapply(df_list, ncol)
)

# Show first 8 column names for each dataset
col_preview <- purrr::imap_dfr(df_list, ~tibble(
  Dataset = .y,
  Columns = paste(head(names(.x), 8), collapse = ", ")
))

knitr::kable(dims, caption = "Dataset Dimensions (Before Selection)")
knitr::kable(col_preview, caption = "Column Preview (First 8 per Dataset)")
```

## 5. Data Selection (Inclusion/Exclusion Rules)

**Rule intent:** keep fields typically useful for health segmentation and modelling; drop redundant or low-quality columns.

- **Keep (if present):** `Indicator`, `Value`, `Value_num`, `Sex`, `Age`, `Region`, `Period`, `Subgroup`.
- **Drop:** free-text/comments, identifiers with zero variance, and columns with **> 40% missing** unless mission–critical.

```{r select-columns}
keep_cols <- c("Indicator","Value","Value_num","Sex","Age","Region","Period","Subgroup")

prune_by_cols <- function(df) {
  cols <- intersect(keep_cols, names(df))
  df[, cols, drop = FALSE]
}

df_sel <- lapply(df_list, prune_by_cols)

# Missing rate per column
col_missing_rate <- function(df) {
  tibble(
    Column     = names(df),
    Missing    = sapply(df, function(x) sum(is.na(x))),
    PctMissing = round(100 * sapply(df, function(x) sum(is.na(x))) / max(1, nrow(df)), 2)
  ) %>% arrange(desc(PctMissing))
}

missing_reports <- purrr::imap(df_sel, ~col_missing_rate(.x) %>% mutate(Dataset = .y))
missing_overview <- dplyr::bind_rows(missing_reports)

# Drop columns with > 40% missing unless they are on a small list of essentials
essentials <- c("Indicator","Value_num")  # keep if available
drop_by_missing <- function(df, threshold = 40){
  rates <- 100 * sapply(df, function(x) sum(is.na(x))) / max(1, nrow(df))
  keep <- names(df)[rates <= threshold | names(df) %in% essentials]
  df[, keep, drop = FALSE]
}

df_sel <- lapply(df_sel, drop_by_missing, threshold = 40)

knitr::kable(head(missing_overview, 30), caption = "Missingness Overview (Top 30 Rows)")
```

## 6. Data Cleaning Process

Actions we perform:
- Remove **exact duplicates**
- **Impute** `Value_num` by **median within (Indicator × Sex)** where possible; otherwise indicator-level or global median
- Optional **winsorization** (clip extreme outliers at 1st/99th percentiles) when appropriate
- Standardize categorical types (factors)

```{r clean-data}
# 6.1 Remove exact duplicates
df_clean <- lapply(df_sel, function(df) df[!duplicated(df), , drop = FALSE])

# 6.2 Impute Value_num
impute_group_median <- function(df) {
  if (!"Value_num" %in% names(df)) return(df)
  # 1) try Indicator x Sex
  if (all(c("Indicator","Sex") %in% names(df))) {
    df <- df %>%
      group_by(Indicator, Sex) %>%
      mutate(Value_num = ifelse(is.na(Value_num),
                                median(Value_num, na.rm = TRUE),
                                Value_num)) %>%
      ungroup()
  }
  # 2) if still NA, try Indicator-only
  if ("Indicator" %in% names(df)) {
    df <- df %>%
      group_by(Indicator) %>%
      mutate(Value_num = ifelse(is.na(Value_num),
                                median(Value_num, na.rm = TRUE),
                                Value_num)) %>%
      ungroup()
  }
  # 3) global fallback
  if (any(is.na(df$Value_num))) {
    gmed <- median(df$Value_num, na.rm = TRUE)
    df$Value_num[is.na(df$Value_num)] <- gmed
  }
  df
}
df_clean <- lapply(df_clean, impute_group_median)

# 6.3 Winsorize (optional) to limit extreme outliers (only if Value_num exists)
winsorize <- function(x, p = c(0.01, 0.99)){
  qs <- quantile(x, probs = p, na.rm = TRUE)
  x <- pmax(x, qs[1]); x <- pmin(x, qs[2]); x
}

df_clean <- lapply(df_clean, function(df){
  if ("Value_num" %in% names(df) && sum(!is.na(df$Value_num)) > 10) {
    df$Value_winz <- winsorize(df$Value_num)
  }
  df
})

# 6.4 Cast categorical columns to factor & build scaled numeric
prep_factors_scale <- function(df){
  for (v in c("Sex","Age","Region","Subgroup","Indicator","Period")){
    if (v %in% names(df)) df[[v]] <- as.factor(df[[v]])
  }
  if ("Value_winz" %in% names(df)) {
    df$Value_scaled <- as.numeric(scale(df$Value_winz))
  } else if ("Value_num" %in% names(df)) {
    df$Value_scaled <- as.numeric(scale(df$Value_num))
  }
  df
}
df_clean <- lapply(df_clean, prep_factors_scale)

# Save interim cleaned CSVs
purrr::iwalk(df_clean, ~write.csv(.x, file.path(out_dir, paste0("clean_", .y, ".csv")), row.names = FALSE))
```

## 7. Attribute / Feature Selection

We use:
- **Spearman** correlation (monotonic) between numeric predictors and `Value_scaled`.
- **Kruskal–Wallis** tests for categorical predictors vs `Value_scaled`.

```{r feature-selection}
# Helper: collect numeric predictors (excluding target derivations)
num_preds <- function(df){
  nums <- names(df)[sapply(df, is.numeric)]
  setdiff(nums, c("Value_num","Value_winz","Value_scaled"))
}

# Spearman
spearman_report <- function(df, name){
  out <- tibble()
  if ("Value_scaled" %in% names(df)) {
    for (nm in num_preds(df)) {
      x <- df[[nm]]
      if (length(unique(na.omit(x))) > 5) {
        ct <- suppressWarnings(cor.test(df$Value_scaled, x, method = "spearman"))
        out <- bind_rows(out, tibble(Predictor = nm, Rho = unname(ct$estimate), P = ct$p.value))
      }
    }
  }
  out %>% mutate(Dataset = name)
}

spearman_all <- purrr::imap_dfr(df_clean, spearman_report)
knitr::kable(head(arrange(spearman_all, P), 15), caption = "Top Numeric Predictors by Spearman (smallest p-values)")

# Kruskal–Wallis for factors
kw_report <- function(df, name){
  cats <- names(df)[sapply(df, is.factor)]
  out <- tibble()
  if ("Value_scaled" %in% names(df)) {
    for (nm in cats) {
      if (length(unique(na.omit(df[[nm]]))) >= 2) {
        kt <- suppressWarnings(kruskal.test(Value_scaled ~ df[[nm]], data = df))
        out <- bind_rows(out, tibble(Variable = nm, KW_ChiSq = unname(kt$statistic), P = kt$p.value))
      }
    }
  }
  out %>% mutate(Dataset = name)
}

kw_all <- purrr::imap_dfr(df_clean, kw_report)
knitr::kable(head(arrange(kw_all, P), 15), caption = "Top Categorical Predictors by Kruskal–Wallis (smallest p-values)")
```

## 8. Data Transformations & Aggregation (One-Hot + Split)

We produce design matrices for modelling and create a **70/30** train/test split, saved as RDS.

```{r transform-and-split}
# One-hot encode via model.matrix; Value_scaled is the numeric response proxy
one_hot_mats <- lapply(df_clean, function(df){
  if (!"Value_scaled" %in% names(df)) return(NULL)
  mm <- model.matrix(Value_scaled ~ . - Value - Value_num - Value_winz, data = df)  # drops raw Value strings
  list(
    X = as.matrix(mm),
    y = as.numeric(df$Value_scaled)
  )
})

dir.create(out_dir, showWarnings = FALSE)
set.seed(42)

split_and_save <- function(name, mat_list){
  if (is.null(mat_list)) return(invisible(NULL))
  n <- length(mat_list$y)
  idx <- sample(seq_len(n), size = max(1, floor(0.7*n)))
  train <- list(X = mat_list$X[idx, , drop = FALSE], y = mat_list$y[idx])
  test  <- list(X = mat_list$X[-idx, , drop = FALSE], y = mat_list$y[-idx])
  saveRDS(train, file.path(out_dir, paste0("train_", name, ".rds")))
  saveRDS(test,  file.path(out_dir, paste0("test_",  name, ".rds")))
  invisible(TRUE)
}

purrr::iwalk(one_hot_mats, split_and_save)
```

## 9. Before/After Quality Snapshot

```{r quality-snapshot}
# Missingness after cleaning
clean_missing_reports <- purrr::imap(df_clean, ~tibble(
  Dataset    = .y,
  MissingTot = sum(sapply(.x, function(c) sum(is.na(c)))),
  Rows       = nrow(.x)
))
knitr::kable(clean_missing_reports, caption = "Missingness After Cleaning (Total NA counts)")

# Simple outlier count (IQR) on Value_num (pre-winsor) if present
iqr_flags <- function(x){
  q1 <- quantile(x, 0.25, na.rm=TRUE); q3 <- quantile(x, 0.75, na.rm=TRUE); i <- q3-q1
  (x < (q1 - 1.5*i)) | (x > (q3 + 1.5*i))
}
outlier_report <- purrr::imap_dfr(df_clean, function(df, nm){
  if (!"Value_num" %in% names(df)) return(tibble(Dataset = nm, Outliers_Value_num = NA_integer_))
  tibble(Dataset = nm, Outliers_Value_num = sum(iqr_flags(df$Value_num), na.rm = TRUE))
})
knitr::kable(outlier_report, caption = "Outliers (IQR) Count on Value_num (Pre-Winsor)")
```

## 10. What to Submit (Milestone 2)

- **Report** (this Rmd rendered to HTML/PDF) covering:
  - Data Description (dims + key fields kept).
  - Data Selection rules and rationale.
  - Data Cleaning (missingness before/after, duplicates removed, outlier policy).
  - Feature Selection results (Spearman & Kruskal lists; 1–2 lines of interpretation each).
  - Transformations (encoding, scaling, winsorization policy) and **Train/Test** procedure.
- **Files in `outputs_m2/`:**
  - `clean_*.csv` for each dataset.
  - `train_*.rds` and `test_*.rds` for modelling in Milestone 3.

```{r done}
cat("\nMilestone 2 complete.\nOutputs written to: ", normalizePath(out_dir), "\n")
```